# âœ‹ SignLang â€” Real-Time Gesture Recognition

A real-time hand gesture recognition web application that detects sign language gestures using a webcam and converts them into text instantly.

Built with a modern full-stack setup using React, FastAPI, and Machine Learning.

---

## ğŸŒ Live Demo

ğŸ‘‰ https://signspeak-realtime.vercel.app/

---

## ğŸ“¸ Preview

![App Preview](./preview.png)

---

## âœ¨ Features

- ğŸ¥ Real-time webcam-based gesture detection  
- âš¡ Instant prediction with minimal latency  
- ğŸ§  Machine Learning model trained on hand landmarks  
- ğŸ”¤ Converts gestures into text output  
- ğŸ“Š Confidence score display  
- ğŸ§¾ Gesture history tracking  
- ğŸŒ‘ Clean modern UI (monochrome theme)

---

## ğŸ—ï¸ Tech Stack

### Frontend
- React (Vite + TypeScript)
- Tailwind CSS
- Framer Motion

### Backend
- FastAPI (Python)
- Uvicorn

### Machine Learning
- MediaPipe (hand tracking)
- Scikit-learn (RandomForest)

---

## âš™ï¸ How It Works

1. Webcam captures hand gesture
2. MediaPipe extracts hand landmarks (21 points)
3. Features are processed into a vector
4. Trained ML model predicts the gesture
5. Output is displayed in real-time

---

## ğŸ“‚ Project Structure

```

ASL_HandGest/
â”‚
â”œâ”€â”€ frontend/        # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â””â”€â”€ public/
â”‚
â”œâ”€â”€ backend/         # FastAPI backend
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ model/
â”‚   â””â”€â”€ utils/
â”‚
â””â”€â”€ README.md

````

---

## ğŸ§ª Setup Instructions

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/your-username/ASL_HandGest.git
cd ASL_HandGest
````

---

### 2ï¸âƒ£ Run Backend

```bash
cd backend
python -m venv web_env
web_env\Scripts\activate   # Windows
source web_env/bin/activate # Mac/Linux

pip install -r requirements.txt
python main.py
```

Backend runs at:

```
http://localhost:8000
```

---

### 3ï¸âƒ£ Run Frontend

```bash
cd frontend
npm install
npm run dev
```

---

### 4ï¸âƒ£ Environment Variable

Create `.env` in frontend:

```
VITE_BACKEND_URL=http://localhost:8000
```

---

## ğŸ“Š Model Details

* Algorithm: Random Forest Classifier
* Input: Hand landmarks (x, y coordinates)
* Features: ~35 extracted features
* Classes: A - Z (expandable to full alphabet)

---

## ğŸš§ Future Improvements

* ğŸ”¤ Full gesture support
* ğŸ¤ Speech output (Text-to-Speech)
* ğŸ“± Mobile optimization
* ğŸ¤– Deep Learning model (CNN/LSTM)
* ğŸŒ Multi-language support

---

## ğŸ’¡ Motivation

This project aims to reduce communication barriers by enabling real-time translation of sign language into text using accessible web technologies.

---

## ğŸ¤ Contributing

Pull requests are welcome. For major changes, please open an issue first.

---

---

## ğŸ‘¨â€ğŸ’» Author

Your Name
GitHub: [https://github.com/github.com/hellykhadka7](https://github.com/hellykhadka7)

---

â­ If you found this useful, consider giving it a star!

```

---

# ğŸ”¥ IMPORTANT IMPROVEMENTS (DO THIS NOW)

### 1. Add Preview Image
Take screenshot and add:
```

frontend/public/preview.png




